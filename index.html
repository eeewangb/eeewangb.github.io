
<!doctype html>
<html>

<head>
<title>Wentong Li|NUAA</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Wentong Li"> 
<meta name="description" content="Wentong Li's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />
<link rel="stylesheet" href="css/style2.css">
<!-- <link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css"> -->
<!-- <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css"> -->


</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Wentong Li 李文通<h1>
				</div>
                 <!--
                <h3>Associate Professor</h3>
	         -->
		<p>
		 College of Artificial Intelligence 			
                </p>
		<p>
                    Nanjing University of Aeronautics and Astronautics (NUAA) </br>
		</p>
		<p>
                    No.29 Jiangjun Road, Nanjing,China </br>
		</p>
		<p>
                    Office: 1205, No.1 Building  </br>
		</p>
		<p>
			<a href="https://github.com/LiWentomng"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=MJjM6BcAAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="20%">
				<img src="assets/imgs/img-lwt.jpg" width="110%"/>
			</td>
		<tr>
	</tbody>
</table>


<h2><em>About Me</em></h2> 

<p style="line-height: 25px;">  
     I am an Associate Professor of the College of Artificial Intelligence at  Nanjing University of Aeronautics and Astronautics.
	Currently, I am a Visiting Scholar at <a href="https://www.polyu.edu.hk/comp/">Department of Computing</a>, <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University</a>, where I collaborate with my Ph.D. advisor, Prof. <a href="http://www4.comp.polyu.edu.hk/~cslzhang/">Lei Zhang</a>.
    Previously, I completed my Ph.D at College of Computer Science and Technology, Zhejiang University, fortunately supervised by Prof.
    <a href="https://person.zju.edu.cn/jkzhu">Jianke Zhu</a> and Prof.
    <a href="http://www4.comp.polyu.edu.hk/~cslzhang/">Lei Zhang </a>(PolyU HK, IEEE Fellow), in June 2024.
    My recent research interests are <i>Visual/Scene Understanding</i>, <i> Embodied AI</i> and <i>Multimodal Large Language Models</i>, particularly in:
	<br>
	<em><b> 1. Fine-grained visual understanding with MLLMs/VLMs</b></em>, including visual referring&grounding for image/video/3D scene. 
	<br>
	<em><b> 2. Embodied scene understanding&interaction</b></em>, including ego-centric analysis, VLN, streaming understanding, reasoning&interaction. 
	<br>
	<em><b> 3. Efficient and effective MLLMs</b></em>, including token reduction, lightweight mllm, efficient high-resolution understanding. 
	<br>
	
	Before, I mainly focus on the field of the techniques for object detection, image segmentaion and their weakly-supervised/label-efficient approaches.  Besides, I am also interested in autonomous driving tasks (HD-Map, 3D-Occupancy, etc.) and 3D reconstruction tasks.
    </p> 
</p>

						
<p style="color: red;"> <em>Looking for self-motivated Masters, Research Interns/Assistants and Ph.Ds (co-supervised), please email me if you have interest. </em> </p>

						
<h2><em>News</em></h2> 
<ul>
	<li>
        [2025.8]: Honored to be invited to serve as <b>Area Chair</b> for <b>ICLR 2026</b>.
    </li>
    <li>
        [2025.8]: Visited The Hong Kong Polytechnic University, where I enjoyed the visit and shared a talk.[<a href="assets/imgs/0809_public.pdf">Slides</a>]
    </li>
    <li>
        [2025.6]: We released the <a href="https://circleradon.github.io/EOCBench/"> EOC-Bench</a>, an <b>object-centric embodied cognition benchmark</b> in dynamic egocentric scenarios.
    </li>
     <li>
        [2025.5]: One paper is accepted by <b>IJCV</b> (TokenPacker, 57 citations at the time of acceptance).
    </li>
     <li>
        [2025.4]: Our VideoRefer and VideoRefer-Bench have been discussed and adopted by NVIDIA & UC Berkely in their <a href="https://arxiv.org/pdf/2504.16072"> DAM </a> work.
    </li>
     <li>
        [2025.2]: Five papers are accepted by <b>CVPR 2025</b> (One <b>Highlight</b>).
    </li>
     <li>
        [2025.2]: We released the <a href="https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K">VideoRefer-700K dataset</a> on HuggingFace. Please see the <a href="https://github.com/DAMO-NLP-SG/VideoRefer">VideoRefer Suite</a> for the details.
    </li>
    <li>
        [2024.12]: Awarded Outstanding Doctoral Dissertation Award of ZJU (浙江大学优秀博士学位论文).
    </li>
    <li>
        [2024.6]: Obtained my Ph.D. degree from ZJU.
    </li>
</ul>

						
<p id="publications">
<h2><em>Preprints</em></h2>
</p>



<div id="pubs"></div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/eocbench.png" width="210" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        Yuqian Yuan*, Ronghao Dang*, Long Li*, <b>Wentong Li*</b>, Diao Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Arxiv, 2506.05287, 2025
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://arxiv.org/abs/2506.05287" target="_blank" rel="noopener">Paper</a> ｜
          <a href="https://circleradon.github.io/EOCBench/" target="_blank" rel="noopener">Project Page</a> |
          <a href="https://github.com/alibaba-damo-academy/EOCBench" target="_blank" rel="noopener">Code</a> <img src="https://github.com/alibaba-damo-academy/EOCBench?style=social" />|
	  <a href="https://huggingface.co/datasets/CircleRadon/EOC-Bench" target="_blank" rel="noopener">HuggingFace</a> | 
      <a href="https://circleradon.github.io/EOCBench/#leaderboard" target="_blank" rel="noopener">LeaderBoard</a> |
		  <a href="https://zhuanlan.zhihu.com/p/1917725466296562079" target="_blank" rel="noopener">中文解读</a> 
          </p>
      </div>
</div>


<div id="pubs"></div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/TDS-CLIP.jpg" width="210" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        TDS-CLIP: Temporal Difference Side Network for Efficient Video Action Recognition
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
	Bin Wang, <b>Wentong Li</b>, Wenqian Wang, Mingliang Gao, Runmin Cong and Wei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Arxiv, 2025
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://arxiv.org/abs/2408.10688" target="_blank" rel="noopener">Paper</a> ｜
          <a href="https://github.com/BBYL9413/TDS-CLIP" target="_blank" rel="noopener">Code</a> <img src="https://github.com/BBYL9413/TDS-CLIP?style=social" /> 
          </p>
      </div>
</div>


						
						
						
<p id="publications">
<h2><em>Selected Publications</em></h2>
</p>

<div id="pubs"></div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/tokenpacker.jpg" width="210" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
         TokenPacker: Efficient Visual Projector for Multimodal LLM
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        <b>Wentong Li*</b>, Yuqian Yuan*, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        IJCV, 2025
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://arxiv.org/abs/2407.02392" target="_blank" rel="noopener">Paper</a> ｜
          <a href="https://github.com/CircleRadon/TokenPacker" target="_blank" rel="noopener">Code</a> 
          <img src="https://img.shields.io/github/stars/CircleRadon/TokenPacker?style=social" /> ｜
	  <a href="https://huggingface.co/collections/sunshine-lwt/tokenpacker-66a234618f0d2327e0cf2cb1" target="_blank" rel="noopener">HuggingFace Model</a> | 
	   <a href="https://zhuanlan.zhihu.com/p/707021763" target="_blank" rel="noopener">中文解读</a>  ｜
	   <a href="https://huggingface.co/papers?date=2024-07-04" target="_blank" rel="noopener">Daily Papers</a> 
          </p>
      </div>
</div>


<div id="pubs"></div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/videorefer.gif" width="210" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
         VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
	Yuqian Yuan, Hang Zhang, <b>Wentong Li</b>, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, Jianke Zhu, Lidong Bing
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        CVPR, 2025
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://arxiv.org/abs/2501.00599" target="_blank" rel="noopener">Paper</a> ｜
          <a href="https://github.com/DAMO-NLP-SG/VideoRefer" target="_blank" rel="noopener">Code</a> 
          <img src="https://img.shields.io/github/stars/DAMO-NLP-SG/VideoRefer?style=social" /> ｜
	   <a href="https://huggingface.co/DAMO-NLP-SG/VideoRefer-7B" target="_blank" rel="noopener">HuggingFace Model</a> |
	   <a href="https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K" target="_blank" rel="noopener">Dataset</a> |
	   <a href="https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-Bench" target="_blank" rel="noopener">VideoRefer-Bench</a> |
	   <a href="https://mp.weixin.qq.com/s/ISYFhXrDGlJS9u3F31Pi3w" target="_blank" rel="noopener">中文解读</a> |
           <a href="https://www.bilibili.com/video/BV1Y5TrzzENM/?spm_id_from=333.788.videopod.sections&vd_source=c0fb9c2678dbcb2446696606d719fc90%20p%3D1&t=3180" target="_blank" rel="noopener">视频解读</a> 
		  
          </p>
      </div>
</div>
					

<div id="pubs"></div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/Inst3d-lmm.png" width="210" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
         Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
	Hanxun Yu*, <b>Wentong Li*</b>, Song Wang,  Junbo Chen, Jianke Zhu
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        CVPR, 2025 (<b>Highlight</b>, 2.9%)
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://www.arxiv.org/abs/2503.00513" target="_blank" rel="noopener">Paper</a> ｜
          <a href="https://github.com/hanxunyu/Inst3D-LMM" target="_blank" rel="noopener">Code</a> 
          <img src="https://img.shields.io/github/stars/hanxunyu/Inst3D-LMM?style=social" /> 
          </p>
      </div>
</div>
		

<div id="pubs"></div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/qyqx.gif" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
         Osprey: Pixel Understanding with Visual Instruction Tuning
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
         Yuqian Yuan*, <b>Wentong Li*</b>, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        CVPR, 2024 (<b>Project Leader</b>)
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/pdf/2312.10032.pdf" target="_blank" rel="noopener">Paper</a> ｜
           <a href="https://github.com/CircleRadon/Osprey" target="_blank" rel="noopener">Code</a>  
	  <img src="https://img.shields.io/github/stars/CircleRadon/Osprey?style=social" /> ｜
	   <a href="http://111.0.123.204:8000/" target="_blank" rel="noopener">Online Demo</a> ｜
           <a href="https://www.youtube.com/watch?v=YsxqHBBnDfk" target="_blank" rel="noopener">Video Demo</a> ｜
	   <a href="https://zhuanlan.zhihu.com/p/673647000" target="_blank" rel="noopener">中文解读</a> |
	   <a href="https://www.bilibili.com/video/BV185411q7Du/?vd_source=c0fb9c2678dbcb2446696606d719fc90 p=1&t=604" target="_blank" rel="noopener">视频解读</a> 
          </p>
      </div>
</div>

									
   <div id="pubs"></div>
      <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/boxinst.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Wentong Li</b>, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Risheng Yu, Xiansheng Hua, Lei Zhang
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            T-PAMI, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10423160" target="_blank" rel="noopener">Paper</a> |
           <a href="https://github.com/LiWentomng/BoxInstSeg" target="_blank" rel="noopener">Code(BoxInstSeg)</a>
	  <img src="https://img.shields.io/github/stars/LiWentomng/BoxInstSeg?style=social" /> ｜
          <a href="https://github.com/LiWentomng/boxlevelset" target="_blank" rel="noopener">Code(MMDet)</a>
	  <img src="https://img.shields.io/github/stars/LiWentomng/boxlevelset?style=social" />
          </p>
      </div>
      </div>

    
    <!--   
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/apro.gif" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        Label-efficient Segmentation via Affinity Propagation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        <b>Wentong Li*</b>, Yuqian Yuan*, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        NeurIPS, 2023
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/pdf/2310.10533.pdf" target="_blank" rel="noopener">Paper</a> ｜
           <a href="https://github.com/CircleRadon/APro" target="_blank" rel="noopener">Code</a>
	  <img src="https://img.shields.io/github/stars/CircleRadon/APro?style=social" /> ｜
	   <a href="https://liwentomng.github.io/apro/" target="_blank" rel="noopener">Project Page</a> |
          <a href="https://zhuanlan.zhihu.com/p/674018681" target="_blank" rel="noopener">中文解读</a>
          </p>
      </div>
    </div>
-->
           
    <!--     
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:7px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/point2mask.png" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
          Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        <b>Wentong Li</b>, Yuqian Yuan, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          ICCV, 2023
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
         <a href="https://arxiv.org/abs/2308.01779" target="_blank" rel="noopener">Paper</a> |
        <a href="https://github.com/LiWentomng/Point2Mask" target="_blank" rel="noopener">Code</a> 
	<img src="https://img.shields.io/github/stars/LiWentomng/Point2Mask?style=social" />
        </p>
    </div>
    </div>
     -->
    <!-- 
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/boxlevelset.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Box-supervised Instance Segmentation with Level Set Evolution
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Wentong Li</b>, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xiansheng Hua, Lei Zhang
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            ECCV, 2022
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          <a href="https://arxiv.org/pdf/2207.09055.pdf" target="_blank" rel="noopener">Paper</a> |
           <a href="https://github.com/LiWentomng/boxlevelset" target="_blank" rel="noopener">Code</a> 
	  <img src="https://img.shields.io/github/stars/LiWentomng/boxlevelset?style=social" />
          </p>
      </div>
      </div>
    -->
 <!--
      <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:5px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/H2RBox.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            H2RBox: Horizontal Box Annotation is All You Need for Oriented Object Detection
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Xue Yang, Gefan Zhang, <b>Wentong Li</b>, Xuehui Wang, Yue Zhou, Junchi Yan
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            ICLR, 2023
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/pdf/2210.06742v5.pdf" target="_blank" rel="noopener">Paper</a>  ｜ 
           <a href="https://github.com/yangxue0827/h2rbox-mmrotate" target="_blank" rel="noopener">Code(MMRotate)</a> 
		  <img src="https://img.shields.io/github/stars/yangxue0827/h2rbox-mmrotate?style=social" /> ｜
          <a href="https://github.com/yangxue0827/h2rbox-jittor" target="_blank" rel="noopener">Code(Jittor)</a> 
		  <img src="https://img.shields.io/github/stars/yangxue0827/h2rbox-jittor?style=social" /> ｜
	    <a href="https://zhuanlan.zhihu.com/p/574337609" target="_blank" rel="noopener">中文解读</a> |
	    <a href="https://www.bilibili.com/video/BV1GD4y1g7s8/" target="_blank" rel="noopener">视频解读</a> 
          </p>
      </div>
      </div>
     
      <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:14px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/OrientedRepPoints.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Oriented RepPoints for Aerial Object Detection
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Wentong Li</b>, Yijie Chen, Kaixuan Hu, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2022
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.pdf" target="_blank" rel="noopener">Paper</a>  | 
           <a href="https://github.com/LiWentomng/OrientedRepPoints" target="_blank" rel="noopener">Code(MMDet)</a> 
	   <img src="https://img.shields.io/github/stars/LiWentomng/OrientedRepPoints?style=social" /> | 
           <a href="https://github.com/open-mmlab/mmrotate" target="_blank" rel="noopener"> Code(MMRotate)</a> 
	  <img src="https://img.shields.io/github/stars/open-mmlab/mmrotate?style=social" />  
          </p>
      </div>
      </div>
    -->
<!--
      <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:18px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/MOD-YOLT.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Multi-scale Object Detection in Satellite Imagery Based on YOLT
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Wentong Li</b>, Wanyi Li, Feng Yang, Peng Wang
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            IGARSS, 2019
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          [<a href="https://www.researchgate.net/profile/Wanyi-Li-7/publication/337504245_Multi-Scale_Object_Detection_in_Satellite_Imagery_Based_On_YOLT/links/5e781c2ca6fdcccd62191490/Multi-Scale-Object-Detection-in-Satellite-Imagery-Based-On-YOLT.pdf" target="_blank" rel="noopener">paper</a>]
          </p>
      </div>
      </div>
-->

<!--
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:9px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/demo-scribble2scene.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Label-efficient Semantic Scene Completion with Scribble Annotations
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
          Song Wang, Jiawei Yu, <b>Wentong Li</b>, Hao Shi, Kailun Yang, Junbo Chen, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            IJCAI, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/pdf/2405.15170" target="_blank" rel="noopener">Paper</a>  |
           <a href="https://github.com/songw-zju/Scribble2Scene" target="_blank" rel="noopener">Code</a>  
	  <img src="https://img.shields.io/github/stars/songw-zju/Scribble2Scene?style=social" />
          </p>
      </div>
      </div>
      -->

	<!--					
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:9px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/mgmap-demo2.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
          Xiaolu Liu, Song Wang, <b>Wentong Li</b>, Ruizi Yang, Junbo Chen, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/pdf/2404.00876.pdf" target="_blank" rel="noopener">Paper</a>  |
           <a href="https://github.com/xiaolul2/MGMap" target="_blank" rel="noopener">Code</a>  
	  <img src="https://img.shields.io/github/stars/xiaolul2/MGMap?style=social" />
          </p>
      </div>
      </div>
    -->

    <!--
					
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:9px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/HASSC-1.jpg" width="200" class="img-bordered" alt="photo">
        </div>

        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with Self-Distillation
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
          Song Wang, Jiawei Yu, <b>Wentong Li</b>, Wenyu Liu, Junbo Chen, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/pdf/2404.11958.pdf" target="_blank" rel="noopener">Paper</a> ｜
         <a href="https://github.com/songw-zju/HASSC" target="_blank" rel="noopener">Code</a>  
	  <img src="https://img.shields.io/github/stars/songw-zju/HASSC?style=social" />
          </p>
      </div>
      </div>
    -->

    <!--
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:8px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/demo-hand.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Qiqun Gan, <b>Wentong Li</b>, Jinwei Ren, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            AAAI, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27946" target="_blank" rel="noopener">Paper</a>｜
           <a href="https://github.com/agnJason/FMHR" target="_blank" rel="noopener">Code</a> 
	<img src="https://img.shields.io/github/stars/agnJason/FMHR?style=social" />	  
          </p>
      </div>
      </div>
    -->


    


    <!-- 
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:19px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/Lidar2map.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
          Song Wang, <b>Wentong Li</b>, Wenyu Liu, Xiaolu Liu, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2023
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
           <a href="https://arxiv.org/abs/2304.11379.pdf" target="_blank" rel="noopener">Paper</a>  ｜
           <a href="https://github.com/songw-zju/LiDAR2Map" target="_blank" rel="noopener">Code</a>  
		  <img src="https://img.shields.io/github/stars/songw-zju/LiDAR2Map?style=social" /> 
          </p>
      </div>
      </div>
        -->



<p id="publications">
<h2><em>Full Publications</em></h2>
</p>

    <blockquote>
        <p><a href="https://arxiv.org/pdf/2504.04801"> OrderChain: A General Prompting Paradigm to Improve Ordinal Understanding Ability of MLLM
            </a> 
        <br />
	Jinhong Wang, Shuo Tong, Dongqi Tang, Weiqiang Wang, <strong>Wentong Li</strong>, Hongxia Xu, Danny Chen, Jintai Chen, Jian Wu <br />
        ICCV, 2025.
    </blockquote> 

    <blockquote>
        <p><a href="https://arxiv.org/abs/2407.02392"> TokenPacker: Efficient Visual Projector for Multimodal LLM
            </a> 
        <br />
        <strong>Wentong Li*</strong>, Yuqian Yuan*, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang <br />
        IJCV, 2025.
    </blockquote> 


    <blockquote>
        <p> Reliable and Calibrated Semantic Occupancy Prediction by Hybrid Uncertainty Learning
            <br />
        Song Wang, Zhongdao Wang, Jiawei Yu,  <strong>Wentong Li</strong>, Bailan Feng,Junbo Chen, Jianke Zhu <br />
        IJCAI, 2025.
    </blockquote> 

    <blockquote>
        <p> Large Models are Good Annotators for Zero-Shot Learning
            <br />
        Qingzhi He, Yizhen Jia, <strong>Wentong Li</strong>, Shengcai Liao, Rong Quan, Tong Cui, Jie Qin <br />
        SIGIR, 2025.
    </blockquote> 


    <blockquote>
		<p><a href="https://arxiv.org/abs/2503.00513"> Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning
			</a> 
		<br />
        Hanxun Yu*, <strong>Wentong Li*</strong>, Song Wang, Junbo Chen, Jianke Zhu <br />
		CVPR, 2025.
	  </blockquote> 

     <blockquote>
        <p><a href="https://arxiv.org/abs/2501.00599"> VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM
        </a> 
		<br />
        Yuqian Yuan, Hang Zhang, <strong>Wentong Li</strong>, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, Jianke Zhu, Lidong Bing <br />
        CVPR, 2025.
     </blockquote> 

     
      <blockquote>
		<p><a href="https://arxiv.org/abs/2504.16023"> PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning
			</a> 
		<br />
        Song Wang, Xiaolu Liu, Lingdong Kong, Jianyun Xu, Chunyong Hu, Gongfan Fang,  <strong>Wentong Li</strong>, Jianke Zhu, Xinchao Wang  <br />
		CVPR, 2025.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2503.23109"> Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction
			</a> 
		<br />
        Xiaolu Liu, Ruizi Yang, Song Wang, <strong>Wentong Li</strong>, Junbo Chen, Jianke Zhu <br />
		CVPR, 2025.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2411.11361"> Scalable Autoregressive Monocular Depth Estimation
			</a> 
		<br />
        Jinhong Wang, Jian Liu, Dongqi Tang, Weiqiang Wang, <strong>Wentong Li</strong>, Danny Chen, Jintai Chen, Jian Wu <br />
		CVPR, 2025.
	  </blockquote> 


      <blockquote>
		<p><a href="https://arxiv.org/abs/2405.15170"> Label-efficient Semantic Scene Completion with Scribble Annotations
			</a> 
		<br />
        Song Wang, Jiawei Yu, <strong>Wentong Li</strong>, Hao Shi, Kailun Yang, Junbo Chen, Jianke Zhu <br />
		IJCAI, 2025.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2312.10032"> Osprey: Pixel Understanding with Visual Instruction Tuning
			</a> 
		<br />
        Yuqian Yuan*, <strong>Wentong Li*</strong>, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu <br />
		CVPR, 2024.
	  </blockquote> 


      <blockquote>
		<p><a href="https://arxiv.org/abs/2404.11958"> Not All Voxels Are Equal: Hardness-aware Semantic Scene Completion with Self-distillation
			</a> 
		<br />
        Song Wang, Jiawei Yu, <strong>Wentong Li</strong>, Wenyu Liu, Xiaolu Liu, Junbo Chen, Jianke Zhu <br />
		CVPR, 2024.
	  </blockquote> 


      <blockquote>
		<p><a href="https://arxiv.org/abs/2404.11958"> MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction
			</a> 
		<br />
        Xiaolu Liu, Song Wang, <strong>Wentong Li</strong>, Ruizi Yang, Junbo Chen, Jianke Zhu <br />
		CVPR, 2024.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2212.01579"> Box2mask: Box-supervised instance segmentation via level-set evolution
			</a> 
		<br />
        <strong>Wentong Li</strong>, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Yu Risheng, Xiansheng Hua, Lei Zhang<br />
		T-PAMI, 2024.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2407.05680"> Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering
			</a> 
		<br />
        Qijun Gan, <strong>Wentong Li</strong>, Jinwei Ren, Jianke Zhu <br />
		AAAI, 2024.
	  </blockquote> 


      <blockquote>
		<p><a href="https://arxiv.org/abs/2310.10533"> Label-efficient Segmentation via Affinity Propagation
			</a> 
		<br />
        <strong>Wentong Li*</strong>, Yuqian Yuan*, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang<br />
		NeurIPS, 2023.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2308.01779"> Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport
			</a> 
		<br />
        <strong>Wentong Li</strong>, Yuqian Yuan, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, Lei Zhang<br />
		ICCV, 2023.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2207.01331"> Improving Nighttime Driving-scene Segmentation via Dual Image-adaptive Learnable Filters
			</a> 
		<br />
        Wenyu Liu, <strong>Wentong Li</strong>, Jianke Zhu, Miaomiao Cui, Xuansong Xie, Lei Zhang<br />
		T-CSVT, 2023.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2304.11379"> LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation
			</a> 
		<br />
        Song Wang, <strong>Wentong Li</strong>, Wenyu Liu, Xiaolu Liu, Jianke Zhu<br />
		CVPR, 2023.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2210.06742"> H2RBox: Horizonal Box Annotation is All You Need for Oriented Object Detection
			</a> 
		<br />
        Xue Yang, Gefan Zhang, <strong>Wentong Li</strong>, Xuehui Wang, Yue Zhou, Junchi Yan<br />
		ICLR, 2023.
	  </blockquote> 

      
      <blockquote>
		<p><a href="https://arxiv.org/abs/2207.09055"> Box-supervised Instance Segmentation with Level Set Evolution
			</a> 
		<br />
        <strong>Wentong Li</strong>, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xian-Sheng Hua, Lei Zhang<br />
		ECCV, 2022.
	  </blockquote> 

      <blockquote>
		<p><a href="https://arxiv.org/abs/2106.00912"> Translational symmetry-aware facade parsing for 3-D building reconstruction
			</a> 
		<br />
        Hantang Liu, <strong>Wentong Li</strong>, Jianke Zhu<br />
		IEEE MultiMedia, 2022.
	  </blockquote> 


      <blockquote>
		<p><a href="https://arxiv.org/abs/2106.00912"> Oriented Reppoints for Aerial Object Detection
			</a> 
		<br />
        <strong>Wentong Li</strong>, Yijie Chen, Kaixuan Hu, Jianke Zhu<br />
		CVPR, 2022.
	  </blockquote> 
      		
<!--
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:8px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/IA-Seg.jpg" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Improving Nighttime Driving-Scene Segmentation via Dual Image-adaptive Learnable Filters
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Wenyu Liu, <b>Wentong Li</b>, Jianke Zhu, Miaomiao Cui, Xuansong Xie, Lei Zhang
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            T-CSVT, 2023
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          [<a href="http://www4.comp.polyu.edu.hk/~cslzhang/paper/IA_Seg_2023.pdf" target="_blank" rel="noopener">paper</a>]
          [<a href="https://github.com/wenyyu/IA-Seg" target="_blank" rel="noopener">code</a>]
          </p>
      </div>
      </div>

      <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:15px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/facade.jpg" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
            Translational Symmetry-Aware Facade Parsing for 3-D Building Reconstruction
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Hantang Liu, <b>Wentong Li</b>, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            IEEE MultiMedia, 2022
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9849000" target="_blank" rel="noopener">paper</a>]
          </p>
      </div>
      </div>
-->


      

<!-- </ul> -->
</script>


</script>

<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>


<h2> <em>Research Experiences</em> </h2>	

<ul>
    <li>
        <a href="https://www.antgroup.com/">Ant Group</a > | HangZhou | Dec.2022 - Sep.2024 </br> 
	Collaborator: <a href="https://sites.google.com/view/li-js">Jianshu Li</a >,<a href="https://openreview.net/profile?id=~Dongqi_Tang2">Dongqi Tang</a >, <a href="https://openreview.net/profile?id=~Jian_liu8">Jian Liu</a > </br>
	Research Intern (Ph.D)
    </li>
    <li>
        <a href="https://damo.alibaba.com/?lang=zh">Alibaba DAMO Academy</a > | HangZhou | July.2020 - Oct.2020 </br>
	Supervisor: Prof. <a href="http://www4.comp.polyu.edu.hk/~cslzhang/">Lei Zhang </a> </br>
	Research Intern (Ph.D)
    </li>
    <li>
        <a href="http://english.ia.cas.cn/">Institution of Automation, CAS </a> | Beijing | July.2018 - June.2019 </br>
	Supervisor: Prof.<a href="https://people.ucas.edu.cn/~wangpengcasia"> Peng Wang </a>, Prof.<a href="https://ia.cas.cn/rcdw/fyjy/202404/t20240422_7129916.html"> Wanyi Li </a> </br>
	Research Assistant (Master)
    </li>
</ul>


						
<h2><em>Honors</em></h2>

<ul>
   <li>
       Outstanding Doctoral Dissertation Award of Zhejiang University, 2024
    </li> 
    <li>
       Excellent Doctoral Graduates of Zhejiang Province, China (Top 1%), 2024
    </li>
    <li>
        Excellent Doctoral Graduates of Zhejiang University, 2024
    </li>
    <li>
        Tencent Scholarship, 2023
    </li>
    <li>
        Five-A Postgraduate Student, 2023
    </li>
    <li>
        Outstanding Postgraduate Student, 2020-2023
    </li>
    <li>
        Longhu Scholarship, 2022
    </li>
    <li>
        First-class Academic Scholarship, 2018-2023
    </li>
    <li>
        National Scholarship, 2016
    </li>
</ul>







<h2><em>Academic Services</em></h2>

<ul>
    <li>
        Conference Reviewer:</br> 
	AAAI2025, ICLR2025, CVPR2025, ICML2025, ICCV2025, NeurIPS2025,  ACM MM2025</br>
        CVPR2024, ICLR2024, ICML2024, ECCV2024, ACM MM2024, NeurIPS2024 </br>
	CVPR2023, ICCV2023, NeurIPS2023, ACM MM2023 </br>
    </li>
    <li>
        Journal Reviewer: </br>
	Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </br>
	International Journal of Computer Vision (IJCV) </br>
	Transactions on Image Processing (TIP) </br>
        Transactions on Circuits and Systems for Video Technology (TCSVT) </br>
	Transactions on Multimedia (TMM) </br>
        Transactions on Geoscience and Remote Sensing (TGRS) </br>
	Pattern Recognition (PR) </br>
	ACM Computing Surveys </br>
        ISPRS Journal of Photogrammetry and Remote Sensing (P&RS) </br>
        Neurcomputing </br>
    </li>
</ul>


<h2><em>Tech. Talks</em></h2>
<ul> 
	<li>
           <em>Efficient Visual Understanding and Interaction with VLMs</em>, PolyU HongKong,  <a href="assets/imgs/0809_public.pdf">slides</a>, 2025/08.
     </li>
	 <li>
            <em>Fine-grained Image Understanding with VLMs</em>, ECNU, <a href="https://github.com/vpx-ecnu">Visual Perception+X(VPX) Group</a>, 2024/09.
         </li>
	 <li>
             <em>Osprey:Pixel Understanding with Visual Instruction Tuning</em>, <a href="https://www.bilibili.com/video/BV185411q7Du/">Video</a>, <a href="https://github.com/cslwt/cslwt.github.io/blob/main/assets/imgs/Osprey-slides.pdf">slides</a>, AI TIME, 2024/01.
         </li>
	 <li>
             <em> Point-supervised Image Segmentation</em>, AntGroup, Machine Intelligence Group, 2023/09.
         </li>
</ul>						

<h2><em>Teaching</em></h2>

<ul>
    <li>
        <em> Intro. to AI: A Foundational Course</em>, NUAA, Fall 2025.
    </li>
    <li>
       <em> Foundations and Frontiers of Multimodal Large Models</em>, NUAA, Spring 2025.
	 </li>
     <li>
        <em>Image Processing and Analysis</em>, Police Brain of Zhejiang Province, Teaching Assistant, Fall 2022.
    </li>
    <li>
        FDS2021: <em>Foundation of Data Structure</em>, Zhejiang University, Teaching Assistant, Fall 2021.
    </li>
</ul>


<h2><em> People</em></h2>
<ul>
	<li> Close Collaborators
	</li>
        <a href="https://yuqianyuan.github.io/">Yuqian Yuan</a>, PhD, ZJU (Research Intern, Alibaba DAMO Academy) </br>
	<a href="https://scholar.google.com/citations?user=Jj0jbL8AAAAJ&hl=zh-CN">Song Wang</a>, PhD, ZJU (Visiting PhD, NUS) </br>
	<a href="https://hanxunyu.github.io/">Hanxun Yu</a>, PhD, ZJU </br>
        <li> Students </li>
	Yizhen Jia, PhD, NUAA (co-supervised) </br>
	Yue Feng, PhD, NUAA  (co-supervised) </br>
	Zihao Xin, PhD, NUAA  (co-supervised) </br>
	Zhiyuan Qi, Master, NUAA (Research Intern, Geely Automobile Research Institute) </br> 
	Jiatong Li, Undergraduate, NUAA </br>
	Yixuan Jiang, Undergraduate, NUAA </br>
	Jinwei Hu, Undergraduate, NUAA </br>
	Zichen Zhao, Undergraduate, NUAA </br>
	Jiangyu Zhou, Undergraduate,  NUAA </br>
</ul>



						
						



<table width="100%"> 
	<tr> 
		<td align="center">&copy; Wentong Li | Last update: Aug. 2025</td>
	</tr> 
</table>

</div>


</body>

</html>
